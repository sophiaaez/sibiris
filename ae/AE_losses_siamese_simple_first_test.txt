trained with learning rate 0.0001, batch size 8, planned epochs 1000 but only took 110 epochs.training_losses
20.908708572387695
25.019960403442383
20.151145935058594
34.09978103637695
34.00263595581055
33.725311279296875
33.63825225830078
33.909950256347656
33.88780975341797
33.83365249633789
33.7391471862793
33.79802322387695
33.73328399658203
33.61854934692383
33.443241119384766
33.971275329589844
34.02455520629883
33.59270477294922
33.4542236328125
34.07025909423828
34.06489181518555
33.92218780517578
34.04826354980469
33.916839599609375
33.9362678527832
33.918556213378906
33.832298278808594
33.66313934326172
33.688194274902344
33.77072525024414
33.85972595214844
33.87593460083008
33.90938949584961
34.02964782714844
33.99296188354492
33.6788215637207
33.69797897338867
33.66450881958008
33.71808624267578
34.20341110229492
34.217430114746094
33.83320236206055
33.91371536254883
33.89060592651367
33.95552062988281
34.05159378051758
34.12279510498047
34.118804931640625
34.00291061401367
34.28324890136719
34.19981384277344
34.07084655761719
34.112117767333984
33.7059440612793
33.773197174072266
34.004974365234375
34.104454040527344
34.029415130615234
34.04008865356445
34.09884262084961
34.0408935546875
33.69240188598633
33.7764892578125
33.86819839477539
33.77926254272461
33.998538970947266
33.979496002197266
34.02840042114258
33.948455810546875
33.73381042480469
33.5974235534668
33.85137176513672
33.71062469482422
33.96621322631836
34.12916946411133
33.82030487060547
33.65187072753906
33.75581359863281
33.73788833618164
34.005149841308594
34.02873611450195
34.006710052490234
34.07554626464844
33.93606185913086
33.9346923828125
34.02688980102539
33.923377990722656
33.7383918762207
33.637428283691406
34.12028503417969
34.149024963378906
33.890159606933594
33.91799545288086
33.88307189941406
34.02602767944336
33.86328125
33.83171081542969
33.887813568115234
33.88758087158203
33.83757400512695
33.86362075805664
33.87208557128906
33.79362487792969
33.82441329956055
33.856544494628906
33.953643798828125
33.848915100097656
33.9000358581543
33.92533493041992
33.9540901184082
33.83245849609375
siamese_losses
278.8670349121094
288.8270568847656
283.8102111816406
277.9967956542969
263.3935241699219
266.62237548828125
250.6965789794922
233.89369201660156
226.82325744628906
226.0529327392578
219.31378173828125
220.5019989013672
196.97523498535156
184.02748107910156
183.03134155273438
182.08944702148438
170.47634887695312
176.73419189453125
171.06849670410156
160.8796844482422
156.99456787109375
158.24668884277344
149.07891845703125
149.67478942871094
144.4846649169922
144.3035430908203
136.0546417236328
132.31504821777344
137.14744567871094
129.88662719726562
113.02786254882812
132.6269073486328
120.06525421142578
124.9073257446289
118.83527374267578
115.44149780273438
118.28878021240234
111.68585968017578
117.40937042236328
115.15382385253906
107.15497589111328
110.87775421142578
102.83380889892578
111.38851928710938
112.67021179199219
117.59239959716797
96.67456817626953
96.77064514160156
99.23066711425781
104.22531127929688
101.65264129638672
90.90648651123047
106.139404296875
101.64227294921875
104.12992095947266
101.45096588134766
validation_losses
37.96397399902344
17.24925422668457
17.310842514038086
17.519887924194336
17.250179290771484
17.556079864501953
17.432043075561523
17.374160766601562
17.565420150756836
17.298608779907227
17.456722259521484
17.41023063659668

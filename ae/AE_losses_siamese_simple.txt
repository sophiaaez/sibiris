trained with learning rate 0.0001, batch size 8, planned epochs 1000 but only took 140 epochs.training_losses
33.78174591064453
31.766584396362305
25.951393127441406
21.21164321899414
20.048492431640625
19.708786010742188
34.54021072387695
34.174888610839844
34.25718307495117
34.288352966308594
34.1243782043457
34.34969711303711
34.23878479003906
34.28057098388672
34.21418762207031
34.244441986083984
34.430442810058594
34.45615005493164
34.419559478759766
34.35552978515625
34.42839813232422
33.8332405090332
33.83885192871094
33.84513854980469
33.87032699584961
33.867759704589844
34.14745330810547
34.16917419433594
34.1314582824707
34.059852600097656
34.11117172241211
33.96225357055664
33.98018264770508
33.97012710571289
34.04868698120117
34.039798736572266
34.08293533325195
34.104434967041016
34.150665283203125
34.0590705871582
34.078651428222656
34.112850189208984
34.06573486328125
34.04606628417969
34.10679244995117
34.0058708190918
33.890220642089844
33.92210006713867
33.91147994995117
33.813087463378906
33.759552001953125
34.22713088989258
34.374244689941406
34.23995590209961
34.17601776123047
34.20283889770508
34.384971618652344
34.31024169921875
34.351619720458984
34.446815490722656
34.39347839355469
34.321929931640625
34.22441101074219
34.36931228637695
34.24717330932617
34.26638412475586
34.18507766723633
34.05059814453125
34.15803909301758
34.183799743652344
34.177364349365234
34.210723876953125
34.07140350341797
34.212738037109375
34.12641525268555
34.16458511352539
33.85613250732422
33.72561264038086
33.94218063354492
33.885189056396484
33.830963134765625
34.10625457763672
34.09992980957031
34.04866027832031
34.15110778808594
34.088191986083984
34.14175796508789
34.14948654174805
34.181304931640625
34.108428955078125
34.15739440917969
34.1571044921875
34.129234313964844
34.2904052734375
34.25335693359375
34.25423812866211
34.55598831176758
34.50813293457031
34.420589447021484
34.48378372192383
34.427879333496094
34.055450439453125
34.05264663696289
34.031341552734375
34.22611999511719
34.058536529541016
34.38821792602539
34.2677116394043
34.36638641357422
34.22570037841797
34.327335357666016
34.60996627807617
34.5097770690918
34.519100189208984
34.57796096801758
34.55583190917969
34.49205017089844
34.586029052734375
34.48276138305664
34.529075622558594
34.50836944580078
34.10343551635742
33.990238189697266
34.089786529541016
34.0474967956543
34.08049011230469
33.9205207824707
33.94077682495117
34.03604507446289
33.880638122558594
34.09406280517578
33.937381744384766
34.12385940551758
34.083744049072266
34.0950927734375
34.05797576904297
33.99803161621094
34.07618713378906
33.97286605834961
33.89056396484375
34.017906188964844
siamese_losses
300.1582946777344
289.47039794921875
272.9786376953125
260.1593017578125
241.1380157470703
231.30458068847656
219.820556640625
219.6813201904297
215.93067932128906
200.83999633789062
201.59176635742188
184.8758087158203
182.1790313720703
179.3786163330078
166.28543090820312
170.9541778564453
165.36874389648438
153.52003479003906
155.96124267578125
155.8675079345703
149.68350219726562
143.1498260498047
155.28164672851562
145.92413330078125
146.23916625976562
133.77854919433594
130.26522827148438
131.9230194091797
128.094970703125
validation_losses
273.70135498046875
513.0654907226562
366.72723388671875
606.5139770507812
225.06263732910156
345.72857666015625
242.40513610839844
260.021728515625
354.20245361328125
322.619873046875
449.62872314453125
525.9362182617188
535.8362426757812
344.3798522949219
772.3074340820312
